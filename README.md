# MLOps Taller 3 - Pipeline Automatizado con Airflow

**Grupo compuesto por Sebastian Rodr√≠guez y David C√≥rdova**

Este proyecto implementa un pipeline completo de Machine Learning Operations (MLOps) que automatiza desde la limpieza de datos hasta el entrenamiento de modelos y despliegue de API, utilizando Apache Airflow como orquestador principal.

## Caracter√≠sticas Principales

- Pipeline completamente automatizado con ejecuci√≥n sin intervenci√≥n manual
- Orquestaci√≥n inteligente del flujo de trabajo con Apache Airflow
- Contenerizaci√≥n completa mediante Docker Compose
- Base de datos MySQL para almacenamiento persistente
- API FastAPI para servicio de predicciones en tiempo real
- Auto-trigger del DAG con activaci√≥n autom√°tica al iniciar
- Dashboard web de Airflow para monitoreo en tiempo real

## Estructura del Proyecto

```
MLOps_Taller3/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ funciones.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ queries.py
‚îÇ   ‚îú‚îÄ‚îÄ fastapi_ready.txt
‚îÇ   ‚îú‚îÄ‚îÄ fastapi.log
‚îÇ   ‚îî‚îÄ‚îÄ orquestador.py
‚îú‚îÄ‚îÄ fastapi/
‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ plugins/
‚îú‚îÄ‚îÄ images/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îî‚îÄ‚îÄ README.md
```

### Descripci√≥n de Componentes

- **dags/**:
  - **orquestador.py**: DAG principal de Airflow que automatiza todo el pipeline de Machine Learning
  - **scripts/funciones.py**: Funciones principales del pipeline (insert_data,clean, read_data, train_model)
  - **scripts/queries.py**: Consultas SQL para creaci√≥n y manipulaci√≥n de tablas en MySQL
  - **fastapi_ready.txt**: Archivo de se√±al para indicar que FastAPI est√° listo
  - **fastapi.log**: Logs del servicio FastAPI

- **fastapi/**:
  - **main.py**: Aplicaci√≥n principal de FastAPI que consume los modelos entrenados
  - **Dockerfile**: Contenerizaci√≥n del servicio API
  - **requirements.txt**: Dependencias espec√≠ficas para el servicio FastAPI

- **models/**:
  - Carpeta compartida que almacena los modelos entrenados en formato pickle (.pkl)
  - Es montada como volumen en todos los contenedores que necesitan acceso a los modelos
  - Contiene archivos como: `RegresionLogistica.pkl`

- **logs/**: Directorio donde Airflow almacena todos los logs de ejecuci√≥n de tareas y DAGs
- **plugins/**: Directorio para plugins personalizados de Airflow (vac√≠o por defecto)
- **images/**: Carpeta para almacenar capturas de pantalla y evidencias del funcionamiento

- **.env**: 
  - Archivo de variables de entorno que configura autom√°ticamente las credenciales de Airflow
  - Elimina la necesidad de configuraci√≥n manual con credenciales predeterminadas (admin/admin)

- **docker-compose.yaml**:
  - Archivo de orquestaci√≥n que define y gestiona todos los contenedores del proyecto
  - Incluye servicios para: Airflow (webserver, scheduler, worker, triggerer), MySQL, Redis, PostgreSQL, FastAPI
  - Contiene el servicio `dag-auto-trigger` que ejecuta autom√°ticamente el pipeline despu√©s del inicio



## Automatizaci√≥n Implementada

### Por qu√© se automatiz√≥

**Problema original:**
- Requer√≠a login manual en Airflow (`admin`/`admin`)
- Necesitaba activar DAGs manualmente
- Requer√≠a trigger manual del pipeline
- Intervenci√≥n humana en m√∫ltiples pasos

**Soluci√≥n automatizada:**
- Zero-touch deployment - Una sola ejecuci√≥n automatiza todo
- Auto-activaci√≥n de DAGs - Se activan autom√°ticamente al iniciar
- Auto-trigger del pipeline - Se ejecuta autom√°ticamente una vez
- Credenciales simplificadas - Admin/admin predeterminado

### Componentes de Automatizaci√≥n

#### Archivo .env - Configuraci√≥n Autom√°tica

```bash
# Variables de entorno para automatizaci√≥n
AIRFLOW_UID=50000
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin
AIRFLOW_PROJ_DIR=.
```

**Funci√≥n:** Elimina la necesidad de configuraci√≥n manual de credenciales.

#### docker-compose.yaml - Orquestaci√≥n Autom√°tica

**Caracter√≠sticas de automatizaci√≥n implementadas:**

```yaml
# DAGs activos por defecto (sin intervenci√≥n manual)
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'false'

# Detecci√≥n r√°pida de cambios en DAGs
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
AIRFLOW__SCHEDULER__PARSING_PROCESSES: 2
```

**Servicio de Auto-Trigger integrado:**
```yaml
dag-auto-trigger:
  command: >
    bash -c "
      echo 'Iniciando auto-trigger del DAG...'
      sleep 120
      echo 'Activando DAG orquestador...'
      airflow dags unpause orquestador || echo 'DAG ya est√° activo'
      echo 'Disparando ejecuci√≥n del DAG...'
      airflow dags trigger orquestador
      echo 'DAG disparado exitosamente!'
    "
```

**Funci√≥n:** Ejecuta autom√°ticamente el pipeline 2 minutos despu√©s del inicio completo.

````markdown
## üîå Conexiones Configuradas

### üê¨ MySQL
```yaml
AIRFLOW_CONN_MYSQL_CONN: 'mysql://my_app_user:my_app_pass@mysql:3306/my_app_db'
````

* Permite conexi√≥n directa de **MySqlHook** y **MySqlOperator**
* Evita hardcodear credenciales en el c√≥digo

### üìÇ FileSensor

```yaml
AIRFLOW_CONN_FS_DEFAULT: 'fs:///'
```

* Usada por **FileSensor** para monitorear archivos del sistema
* √ötil para pipelines basados en llegada de archivos

```
```









#### DAG Modificado - orquestador.py

**Configuraci√≥n para auto-activaci√≥n:**
```python
with DAG(
    dag_id="orquestador",
    schedule_interval=None,          # Ejecuci√≥n controlada autom√°ticamente
    catchup=False,
    is_paused_upon_creation=False,   # CLAVE: DAG activo desde creaci√≥n
    tags=['ml', 'penguins', 'auto-execution']
) as dag:
```

**Funci√≥n:** Garantiza que el DAG est√© listo para ejecuci√≥n autom√°tica.


## Flujo del Pipeline Automatizado

### Secuencia de Ejecuci√≥n Autom√°tica:

1. docker-compose up
2. Servicios iniciando (MySQL + Redis + PostgreSQL)
3. Airflow Webserver + Scheduler
4. DAG auto-activo
5. Auto-trigger despu√©s de 120 segundos
6. Pipeline ML ejecut√°ndose autom√°ticamente


## DAG Orquestador (`orquestador.py`)

Este DAG orquesta todo el flujo de **ETL + entrenamiento de modelo** de ping√ºinos:

1. **Preparaci√≥n de la base de datos**
   - Elimina tablas previas (`penguins_raw` y `penguins_clean`) si existen.
   - Crea las tablas necesarias para datos crudos y limpios.

2. **Carga y limpieza de datos**
   - Inserta datos de ping√ºinos en la tabla `penguins_raw`.
   - Limpia y transforma los datos (One-Hot Encoding, manejo de NaN) y los inserta en `penguins_clean`.

3. **Entrenamiento del modelo**
   - Usa los datos limpios para entrenar un modelo de **Regresi√≥n Log√≠stica**.
   - Guarda el modelo entrenado en `/opt/airflow/models/RegresionLogistica.pkl`.

4. **Validaci√≥n del modelo**
   - Un `FileSensor` verifica que el archivo del modelo exista antes de finalizar el pipeline.

---

### Resumen del flujo

```
delete_table + delete_table_clean
         ‚Üì
  create_table_raw
         ‚Üì
 create_table_clean
         ‚Üì
   insert_data
         ‚Üì
    read_data
         ‚Üì
   train_model
         ‚Üì
wait_for_model_file (FileSensor)
```





**Resultado final:**  
Se obtiene un modelo de clasificaci√≥n entrenado y validado autom√°ticamente, listo para ser consumido desde FastAPI.








## Instrucciones de Ejecuci√≥n

### Preparaci√≥n Inicial

```bash
# Clonar el repositorio
git clone https://github.com/DAVID316CORDOVA/MLOps_Taller3.git
cd MLOps_Taller3

# Limpiar entorno previo (si existe)
docker-compose down -v
docker system prune -f
```

### Ejecuci√≥n Completamente Autom√°tica (Recomendado)

```bash
# Despu√©s de la preparaci√≥n inicial, simplemente:
docker-compose up
```

**Qu√© sucede autom√°ticamente:**
- Se crean todos los contenedores necesarios
- Airflow inicia con credenciales admin/admin
- DAG se activa autom√°ticamente
- Pipeline se ejecuta una vez autom√°ticamente despu√©s de 2 minutos
- FastAPI queda disponible con modelo entrenado

### Ejecuci√≥n en Background

```bash
# Para ejecutar en segundo plano
docker-compose up -d

# Ver logs en tiempo real
docker-compose logs -f dag-auto-trigger
```

### Verificaci√≥n Manual del Estado

```bash
# Verificar que Airflow est√© disponible
curl -f http://localhost:8080/health

# Verificar estado de contenedores
docker-compose ps

# Acceder a la interfaz web
# http://localhost:8080 (admin/admin)
```

## Acceso a Servicios

| Servicio | URL | Credenciales | Descripci√≥n |
|----------|-----|--------------|-------------|
| **Airflow Web** | http://localhost:8080 | admin/admin | Dashboard del pipeline |
| **FastAPI Docs** | http://localhost:8000/docs | - | API de predicciones |
| **MySQL** | localhost:3306 | my_app_user/my_app_pass | Base de datos |
| **Flower (opcional)** | http://localhost:5555 | - | Monitor de Celery |

## Ejecuci√≥n del Proyecto

### 1. Levantamiento de la aplicaci√≥n
![Inicio del sistema](./images/compose.jpg)

### 2. Login de Airflow
![Inicio del sistema](./images/login.jpg)

### 3. Ejecuci√≥n Autom√°tica del Pipeline - DAG Auto-Activo
![Inicio del sistema](./images/dag.jpg)

## 4. Visualizaci√≥n todos los tasks de Airflow ejecut√°ndose automaticamente
![Inicio del sistema](./images/orquesta.jpg)

## 5. Visualizaci√≥n del correcto funcionamiento de la interfaz gr√°fica de FASTAPI 
![Inicio del sistema](./images/fastapi.jpg)


## 6. Predicci√≥n usando el modelo generado autom√°ticamente por AirFlow
![Inicio del sistema](./images/fastapi_prediction.jpg)


## Funciones T√©cnicas Implementadas

### funciones.py - L√≥gica del Pipeline

```python
def insert_data():
    """Inserta datos de Palmer Penguins en MySQL"""
    # Carga dataset Palmer Penguins
    # Limpia valores nulos y NaN
    # Inserta registros en tabla MySQL `penguins_raw`

def clean(df):
    """Limpia y transforma los datos"""
    # Elimina registros con valores nulos
    # Aplica One-Hot Encoding para variables categ√≥ricas (island, sex)
    # Convierte columnas booleanas a enteros
    # Transforma species a valores num√©ricos (1=Adelie, 2=Chinstrap, 3=Gentoo)
    # Retorna DataFrame listo para almacenar en `penguins_clean`

def read_data():
    """Lee y procesa datos desde MySQL"""
    # Extrae registros desde tabla `penguins_raw`
    # Aplica limpieza y codificaci√≥n con `clean()`
    # Inserta datos transformados en tabla `penguins_clean`

def train_model():
    """Entrena y guarda un modelo de Regresi√≥n Log√≠stica"""
    # Carga datos desde tabla `penguins_clean`
    # Divide dataset en entrenamiento y prueba
    # Entrena modelo de clasificaci√≥n
    # Eval√∫a desempe√±o con m√©tricas (accuracy, confusion matrix, classification report)
    # Guarda modelo en `/opt/airflow/models/RegresionLogistica.pkl`

def start_fastapi_server():
    """Prepara entorno FastAPI para servir el modelo"""
    # Verifica existencia del modelo entrenado
    # Configura aplicaci√≥n FastAPI ubicada en `/opt/airflow/dags/fastapi_app.py`
    # Genera archivo de estado `fastapi_ready.txt`
    # Sugiere comando de despliegue con uvicorn

```

### queries.py - Consultas SQL

```sql
DROP_PENGUINS_TABLE = """
DROP TABLE IF EXISTS penguins_raw;
"""

DROP_PENGUINS_CLEAN_TABLE = """
DROP TABLE IF EXISTS penguins_clean;            
 """


CREATE_PENGUINS_TABLE_RAW = """ CREATE TABLE penguins_raw (
            species VARCHAR(50) NULL,
            island VARCHAR(50) NULL,
            bill_length_mm DOUBLE NULL,
            bill_depth_mm DOUBLE NULL,
            flipper_length_mm DOUBLE NULL,
            body_mass_g DOUBLE NULL,
            sex VARCHAR(10) NULL,
            year INT NULL
        )
        """

CREATE_PENGUINS_TABLE_CLEAN = """ CREATE TABLE penguins_clean (
    species INT NULL,
    bill_length_mm DOUBLE NULL,
    bill_depth_mm DOUBLE NULL,
    flipper_length_mm DOUBLE NULL,
    body_mass_g DOUBLE NULL,
    year INT NULL,
    island_Biscoe INT NULL,
    island_Dream INT NULL,
    island_Torgersen INT NULL,
    sex_female INT NULL,
    sex_male INT NULL
        );      
        """
"""

```









## Conclusiones

Este proyecto implementa un pipeline MLOps completamente automatizado que:

- Elimina intervenci√≥n manual en el proceso de entrenamiento
- Proporciona un sistema reproducible y confiable
- Integra todas las fases del ciclo de vida del modelo
- Ofrece monitoreo y trazabilidad completa
- Reduce significativamente el tiempo de despliegue

La automatizaci√≥n establecida proporciona una base s√≥lida para operaciones de Machine Learning en producci√≥n, minimizando errores humanos y maximizando la eficiencia operacional.

---

**Desarrollado por:**
- Sebastian Rodr√≠guez  
- David C√≥rdova

**Proyecto:** MLOps Taller 3 - Pipeline Automatizado  
**Fecha:** Septiembre 2025
